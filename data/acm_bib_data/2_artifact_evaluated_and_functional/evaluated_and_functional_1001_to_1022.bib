@software{10.5281/zenodo.1299357,
author = {Wu, Meng and Guo, Shengjian and Schaumont, Patrick and Wang, Chao},
title = {[ISSTA '18 Artifact Evaluation] Eliminating Timing Side-Channel Leaks using Program Repair},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.1299357},
abstract = {
    <p>This is an artifact evaluation release for ISSTA 18' paper "Eliminating Timing Side-Channel Leaks using Program Repair" The artifact is in an open virtual machine format which can be open with VirtualBox or VMware. To log into the virtual machine, use username: ae, pwd: 123</p>
},
keywords = {program repair, timing side channel}
}

@software{10.5281/zenodo.1306224,
author = {Tabareau, Nicolas and Tanter, \'{E}ric and Sozeau, Matthieu},
title = {Coq Development: Equivalences for Free: Univalent Parametricity for Effective Transport},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.1306224},
abstract = {
    <p>Coq source files accompanying the ICFP 2018 paper "Equivalences for Free: Univalent Parametricity for Effective Transport", by Tabareau, Tanter, Sozeau.</p>
},
keywords = {Coq, parametricity, Univalence}
}

@software{10.1145/3234985,
author = {Jiang, Jiajun and Xiong, Yingfei and Zhang, Hongyu and Gao, Qing and Chen, Xiangqun},
title = {SimFix},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3234985},
abstract = {
    <p>Replication package for paper "Shaping Program Repair Space with Existing Patches and Similar Code"</p>
},
keywords = {Automated program repair, code adaptation, code differencing}
}

@software{10.1145/3234988,
author = {Fourtounis, George and Kastrinis, George and Smaragdakis, Yannis},
title = {Artifact for Article: "Static Analysis of Java Dynamic Proxies"},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3234988},
abstract = {
    <p>This artifact contains the software and benchmarks needed to evaluate the article "Static Analysis of Java Dynamic Proxies". The artifact contains the Doop analysis framework, a suite of XCorpus programs containing proxies, and two case studies (okhttp, guice), all packaged as a Docker archive.</p>
},
keywords = {Doop, dynamic proxies, reflection, static analysis}
}

@software{10.1145/3234991,
author = {Grech, Neville and Fourtounis, George and Francalanza, Adrian and Smaragdakis, Yannis},
title = {A docker container with all necessary software},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3234991},
abstract = {
    <p>A docker container with all necessary software: - Doop - HeapDL - Benchmarks (dacapo-bach) - Memory dumps - Scripts for running benchmarks and collecting results</p>
},
keywords = {Program analysis}
}

@software{10.5281/zenodo.1296310,
author = {Bl\"{a}ser, Luc},
title = {HSR Parallel Checker},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.1296310},
abstract = {
    <p>Checker tool described in the paper. Please follow Installation.pdf for the tool setup and further instruction steps.</p>
},
keywords = {.NET, C#, data race, deadlock, static checker}
}

@software{10.5281/zenodo.1301239,
author = {DeFreez, Daniel and Thakur, Aditya V. and Rubio-Gonz\'{a}lez, Cindy},
title = {func2vec-fse2018-artifact},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.1301239},
abstract = {
    <p>Artifact with source for running Func2vec and link to data for reproducing experiments.</p>
},
keywords = {error handling, program analysis, program comprehension, program embeddings, specification mining}
}

@software{10.5281/zenodo.1218718,
author = {Liu, Peizun and Wahl, Thomas},
title = {CUBA: Interprocedural Context-Unbounded Analysis of Concurrent Programs (Artifact)},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.1218718},
abstract = {
    <p>This is the artifact package for the article: CUBA: Interprocedural Context-Unbounded Analysis of Concurrent Programs. It includes: 1. The tool implemented in the article. In particular, -- the source code, -- executable binaries, -- an installation guide, -- documentation. 2. A brief introduction to the syntax used in our input programs. 3. A brief tutorial. 4. A set of benchmarks used and a guide to run the experiments.</p>
},
keywords = {Concurrent Program, Context Bound, Interprocedural Analysis, Recursion, Stack}
}

@software{10.5281/zenodo.1218771,
author = {Chen, Dong and Liu, Fangzhou and Ding, Chen and Pai, Sreepathi},
title = {Software Artifact for Locality Analysis through Static Parallel Sampling},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.1218771},
abstract = {
    <p>We provide the Static Parallel Sampling (SPS) artifact that contains code, testing shell scripts and Python scripts for plotting result. We extracted the minimal code needed from the newest version of the Static Parallel Sampling tools and provide a Dockerfile to generate one working testing environment. This given artifact can reproduce all our results in Figures 5-7 (Miss ratio curve, overhead and parallel execution) in the evaluation section of the paper.</p>
},
keywords = {locality, program specialization, Static analysis}
}

@software{10.1145/3211990,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {Replication Package for Article "To-Many or To-One? All-in-One! Efficient Purely Functional Multi-maps with Type-Heterogeneous Hash-Tries"},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211990},
abstract = {
    <p># Getting Started Guide The evaluation of our PLDI'18 paper entitled _To-many or To-one__?__ All-in-one! --- Efficient Purely Functional Multi-Maps with Type-Heterogeneous Hash-Tries_ consists of microbenchmarks and a case study in program analysis, both benchmarks execute fully automated. ## Requirements We assume basic familiarity with UNIX terminals and command line tools. The artifact was tested under UNIX operating systems (i.e., Ubuntu Linux 16.04.3.LTS and Apple macOS). The artifact requires the following resources to run: * Command line tools: * java (version 8), * maven, * make, * ant, * R and RScript * Internet connection (for automatically downloading dependencies). The execution time benchmarks are configured to use heap sizes of 8GB, whereas the memory measurments use 16GB. Thus computers or virtual machiens with at least 8GB RAM are recommended. For ease of use, we created a virtual machine with tools, benchmarks, and data setup. As basis we used the **Ubuntu Linux 16.04.3.LTS** distribution, installed in a [**VirtualBox**](https://www.virtualbox.org) virtual machine (version 5.2.6) with the **Oracle VM VirtualBox Extension Pack** installed. Go to [www.virtualbox.org](www.virtualbox.org) for downloading the software and installation instructions. The virtual machine has the following packages installed in order to satisfy our requirements: * `sudo apt install git` * `sudo apt install default-jdk` * `sudo apt install maven` * `sudo apt install openssh-server` * `sudo apt install r-base r-base-dev` ## SSH access to Virtual Machine For more convenient usage of the artifact, the authors are asked to setup remote access via terminal according to the stackoverflow answer [How to SSH to a VirtualBox guest externally through a host__?__](https://stackoverflow.com/questions/5906441/how-to-ssh-to-a-virtualbox-guest-externally-through-a-host). Afterwards, users of the artifact can log into the virtual machine with the following command line: &gt; ssh -p 3022 axiom@localhost When prompted for a password, use the password "axiom" (i.e., the same as the username that is already provided in the commannd line). ## Executing the Benchmarks and Data Post-Processing Pipeline Thh execution of benchmark referred to in the evaluation Sections 4, 5, and 6 of the paper, are entriely automated. The data post-processing of the data used in Sections 4 and 5 (cf. Figures 4, 5 and 6) are automated as well. Getting started with reproducing our resutls requires the execution of following commands in a console/terminal after being logged into our virtual machine: Moving into the artifacts directory: &gt; cd pldi18-artifact Setting up and compiling the artifacts: &gt; make prepare (To manually inspect what the **make** commands do, have a look at *pldi18-artifact/Makefile*.) ### (Quickly) Evaluating Archived Data concerning Sections 4 and 5 Next, one can re-generate the boxplots of the Figures 4, 5 and 6 (page 8 of the paper) based the logs that we obtained when executing the benchmarks ourselves. Our cached results are contained in the folder *data/20170417_1554*. The folder contains the following files: * **results.all-20170417_1554.log**: comma-separated values (CSV) file containing microbenchmark results of runtimes of individual operations * **map_sizes_heterogeneous_exponential_32bit_20170417_1554**: CSV file containing memory footprints in a 32-bit JVM setting. * **map_sizes_heterogeneous_exponential_64bit_20170417_1554**: CSV file containing memory footprints in a 64-bit JVM setting. * **hamt-benchmark-results-20170417_1554.tgz**: an archieve containing the files mentioned above verbose console output from running the benchmarks. The folder addionally contains three PDFs that were generated from the data files referenced above. The files correspond directly to the boxplots of Figures 4, 5, and 6 of page 8 of the paper. The boxplots are named **all-benchmarks-vf_champ_multimap_hhamt_by_vf_(scala|clojure|champ_map_as_multimap-map)-boxplot-speedup.pdf** and were generated with the following command, which should finish in a few seconds: &gt; make postprocessing_cached The reviewer may delete the three PDFs and re-generate them by with the command. An R script that is located under *benchmark/resources/r/benchmarks.r* performs the data post-processing and generation of the plots. ### (Relatively Quickly) Re-executing the whole Pipeline with a Reduced Dataset concerning Sections 4 and 5 The following commands re-execute the benchmarks for a reduced data set (with not statistically significant results) for the data structure sizes 16, 2048 and 1048576: Running reduced set of microbenchmarks: &gt; make run_microbenchmarks_short Benchmarking should approximately be finished in 15 to 30 minutes. After the benchmarks finished, the *data* directory should contain a new timestamped sub-folder with the benchmark results. After executing the following post-processing command, the folder should once again contain three PDFs / figures with boxplots: Running result analysis and post-processing: &gt; make postprocessing Note, that the results are neither statistically significant, nor do the cover a representative set of data structures size or data points in general. Nevertheless, the shape the boxplots may be similar to the figures in the paper. ### (Extremely Long) Re-executing the whole Pipeline with the Full Dataset concerning Sections 4 and 5 Running reduced set of microbenchmarks: &gt; make run_microbenchmarks Running result analysis and post-processing: &gt; make postprocessing Benchmarking the full data set can take up to two days of processing and should be performed on a dedicated machine without load or extraneous processes running in order to yield reliable results (see paper section about experimental setup and related work). ### (Relatively Quickly) Re-executing the Static Analysis Case Study concerning Section 6 Running case study benchmarks: &gt; make run_static_analysis_case_study Executing the case study benchmark I guess takes approximatley 1 to 2 hours. The benchmark results should be shown in tabular form in the terminal at the end of the benchmark run, and also serialized to disk (to a CSV file named *results.all-real-world-$TIMESTAMP.log*). Post-processing the data of this benchmark was not automated, and instead done by hand. However, it should be observable that the benchmark results for CHAMP and AXIOM are roughly the same. Note that abbreviations used in the benchmark setup do not match the names in the paper, i.e., CHART is used for CHAMP, and VF_CHAMP_MULTIMAP_HHAMT is used for AXIOM. The results cover the first three columns of Table 1. Colums 3-6 of Table 1 were also extracted manually with an instrumented version of the benchmark (cf. file *benchmark/src/main/java/dom/multimap/DominatorsSetMultimap_Default_Instrumented.java*). ## Other Relevant Items of our Artifact Our AXIOM hash trie implementations can be found under *code/capsule-experimental/src/main/java/io/usethesource/capsule/experimental/multimap/TrieSetMultimap_HHAMT.java*, for people interested in manually inspecting the implementation. The packages *benchmark/src/main/scala/io/usethesource/criterion/impl/persistent/scala* and *benchmark/src/main/java/io/usethesource/criterion/impl/persistent/clojure* contain simple interface facades that enables cross-library benchmarks under a common API. The benchmark implementations can be found in the *benchmark* project. File *benchmark/src/main/java/dom/multimap/DominatorsSetMultimap_Default.java* implements the real-word experiment (Section 6 of the paper), and file *benchmark/src/main/java/dom/multimap/DominatorsSetMultimap_Default_Instrumented.java* was used to extracting further statistics (colums 4-6 of Table 1). File *JmhSetMultimapBenchmarks.java* measures the runtimes of individual operations, whereas *CalculateHeterogeneousFootprints.java* performs footprint measurements (cf. page 8, Figures 4, 5, and 6). Note that the Java benchmark classes contain default parameters for their invocation, however the actual parameters are set in *runMicrobenchmarks.sh* and *runStaticProgramAnalysisCaseStudy.sh*.</p>
},
keywords = {Data structures, functional programming, graph, hashtable, JVM., many-to-many relation, multi-map, optimization, performance, persistent data structures}
}

@software{10.1145/3212005,
author = {Pombrio, Justin and Krishnamurthi, Shriram},
title = {PLDI 2018 Artifact for Inferring Type Rules for Syntactic Sugar},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3212005},
abstract = {
    <p>This is the artifact for the paper "Inferring Type Rules for Syntactic Sugar" by Justin Pombrio and Shriram Krishnamurthi. Type systems and syntactic sugar are both valuable to programmers, but sometimes at odds. While sugar is a valuable mechanism for implementing realistic languages, the expansion process obscures program source structure. As a result, type errors can reference terms the programmers did not write (and even constructs they do not know), baffling them. The language developer must also manually construct type rules for the sugars, to give a typed account of the surface language. We address these problems by presenting a process for automatically reconstructing type rules for the surface language using rules for the core. We have implemented this theory, and show several interesting case studies.</p>
},
keywords = {Macros, Programming Languages, Resugaring, Syntactic Sugar, Type Systems}
}

@software{10.1145/3211981,
author = {Panchekha, Pavel and Geller, Adam T. and Ernst, Michael D. and Tatlock, Zachary and Kamil, Shoaib},
title = {Replication of Evaluation for VizAssert},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211981},
abstract = {
    <p>Fully reproduces the experiments that are introduced in the paper, including: automatically verifying (or producing counterexamples for) 14 industry-standard assertions on 51 professionally-designed web pages; and testing VizAssert's formalization of line height, margin collapsing, and floating layout against Mozilla Firefox.</p>
},
keywords = {accessibility, CSS, layout, semantics, SMT, usability, verification}
}

@software{10.1145/3211984,
author = {Brutschy, Lucas and Dimitrov, Dimitar and M\"{u}ller, Peter and Vechev, Martin},
title = {C4 Tool Source Code},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211984},
abstract = {
    <p>This is the source code for the C4 tool that we developed and reported in our "Static Serializability Analysis for Causal Consistency" PLDI '18 paper. For more information, check out the project home page at http://ecracer.inf.ethz.ch/.</p>
},
keywords = {Atomic Visibility, Causal Consistency, Serializability, Static Analysis}
}

@software{10.1145/3211988,
author = {Sanchez-Stern, Alex and Panchekha, Pavel and Lerner, Sorin and Tatlock, Zachary},
title = {Herbgrind Sources &nbsp;at Time Of Submission},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211988},
abstract = {
    <p>A source archive of Herbgrind, and the version of Herbie used in the paper eval. A more up-to-date version of Herbgrind can be found at github.com/uwplse/herbgrind.git</p>
},
keywords = {debugging, dynamic analysis, floating point}
}

@software{10.1145/3211997,
author = {Gehr, Timon and Misailovic, Sasa and Tsankov, Petar and Vanbever, Laurent and Wiesmann, Pascal and Vechev, Martin},
title = {Artifact for the PLDI'18 paper "Bayonet: Probabilistic Inference for Networks"},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211997},
abstract = {
    <p>This is the version of Bayonet that was used to generate the results in the paper "Bayonet: Probabilistic Inference for Networks" Find the current version of Bayonet at: http://bayonet.ethz.ch</p>
},
keywords = {Computer Networks, Probabilistic Programming}
}

@software{10.1145/3211982,
author = {Vilk, John and Berger, Emery D.},
title = {Software Artifact for BLeak: Automatically Debugging Memory Leaks in Web Applications},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211982},
abstract = {
    <p>The artifact consists of a VirtualBox VM image (OVA file) containing the BLeak software and source code, evaluation data, instructions, and all open source evaluation applications and input files.</p>
},
keywords = {BLeak, debugging, JavaScript, leak debugging, leak detection, memory leaks, web applications, web browsers}
}

@software{10.1145/3211994,
author = {Wang, Di and Hoffmann, Jan and Reps, Thomas},
title = {PMAF: An Algebraic Framework for Static Analysis of Probabilistic Programs},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211994},
abstract = {
    <p>PMAF is an algebraic framework for static analysis of probabilistic programs. PMAF takes an interpretation of a pre-Markov algebra (PMA) and a program as its input, and then performs the analysis specified by the algebra on the program automatically via a fixpoint computation. We have instantiated PMAF on three PMAs, obtaining tolls for: Bayesian inference, the Markov decision problem, and linear expectation-invariant analysis.</p>
},
keywords = {Expectation invariant, Pre-Markov algebra, Probabilistic program, Program analysis}
}

@software{10.1145/3212003,
author = {Zhu, He and Magill, Stephen and Jagannathan, Suresh},
title = {Artifact for paper: A Data-Driven CHC Solver (PLDI 2018)},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3212003},
abstract = {
    <p>LinearArbitrary-SeaHorn: A CHC solver for LLVM-based languages</p>
},
keywords = {Constrained Horn Clauses (CHCs), Data-Driven Analysis, Invariant Inference, Program Verification}
}

@software{10.1145/3211992,
author = {Lee, Woosuk and Heo, Kihong and Alur, Rajeev and Naik, Mayur},
title = {Replication Package for PLDI'18 of Euphony},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211992},
abstract = {
    <p>This is the replication package for our paper titled "Accelerating Search-Based Program Synthesis using Learned Probabilistic Models". The package includes a virtual machine image along with instructions how to use our tool and reproduce the claims in our paper. Using this package, one can reproduce Table 4, 5, 6, 7 and Figure 8 in the paper.</p>
},
keywords = {Domain-specific languages, Statistical methods, Synthesis, Transfer learning}
}

@software{10.1145/3190500,
author = {Zhou, Qing and Li, Lian and Wang, Lei and Xue, Jingling and Feng, Xiaobing},
title = {Replication Package for Article: May-Hppen-in-Parallel Analysis with Static Vector Clocks},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3190500},
abstract = {
    <p>This is a virtual box image. You can use it to run the default data race detector LDruid with Static Vector Clocks. Static vector clocks are used to do MHP analysis. The image does not consist of source code, and we plan to open source in future. The relevant algorithm and experimental results can be found in paper CGO2018: MHP analysis with Static Vector Clocks.</p>
},
keywords = {Data race detector, MHP, Static vector clocks}
}

@software{10.1145/3190495,
author = {Xie, Biwei and Zhan, Jianfeng and Liu, Xu and Gao, Wanling and Jia, Zhen and He, Xiwen and Zhang, Lixin},
title = {Replication Package for Article, CVR: Efficient Vectorization of SpMV on X86 Processors},
year = {2018},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3190495},
abstract = {
    <p>The artifact contains the source code of SpMV using CVR after converting a sparse matrix from CSR to CVR. It can support the experiment results in section 8 of our CGO'2018 paper, CVR: Efficient SpMV Vectorization on X86 Processors. To validate the results, build CVR and run the benchmarks with provided scripts. This artifact provides general instructions to evaluate CVR, while more details are provided on Github (
 <ext-link xlink:href="https://github.com/puckbee/CVR">
  https://github.com/puckbee/CVR)
 </ext-link>.</p>
},
keywords = {Knights Landing, Many core, SIMD, SpMV}
}

@software{10.1145/3190496,
author = {Kruse, Michael and Grosser, Tobias},
title = {CGO '18 Artifact for DeLICM},
year = {2017},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3190496},
abstract = {
    <p>The artifact contains the scripts used to obtain the data from the article, including the results we obtained ourselves.</p>
},
keywords = {experiment, LLVM, Polly, result}
}

