The 55th IEEE/ACM International Symposium on Microarchitecture® (MICRO) will conduct artifact evaluation (AE) for the second time. AE has become a common practice in the systems community (OSDI, PLDI, PACT, MLSys), and has recently been successfully introduced to the architecture community, with ASPLOS conducting AE in the last three years, and MICRO doing so as well in 2021. We invite the authors of accepted MICRO 2022 papers to submit their artifacts to be assessed based on the ACM Artifact Review and Badging policy. Note that this submission is voluntary and will not influence the final decision regarding the papers.

Process
The authors of accepted papers at MICRO 2022 will be invited to submit their artifacts according to the established submission guidelines followed by previous conferences. Submission will be then reviewed according to the reviewing guidelines:



 Artifact evaluation
 This document provides the guidelines to evaluate artifacts at ACM and IEEE conferences.

 Overview
 Shortly after the artifact submission deadline, the AE committee members will bid on artifacts they would like to evaluate based on their competencies and the information provided in the artifact abstract such as software and hardware dependencies while avoiding possible conflicts of interest.

 Within a few days, the AE chairs will make the final selection of evaluators to ensure at least two or more evaluators per artifact.

 Evaluators will then have approximately 1 months to review artifacts via HotCRP, discuss with the authors about all encountered issues and help them fix all the issues. Remember that our philosophy of artifact evaluation is not to fail problematic artifacts but to help the authors improve their public artifacts, pass evaluation and improve their Artifact Appendix.

 In the end, the AE chairs will decide on a set of the standard ACM reproducibility badges (see below) to award to a given artifact based on all reviews and the authors' responses. Such badges will be printed on the 1st page of the paper and will be available as meta information in the ACM Digital Library

 Authors and reviewers are encouraged to check the AE FAQ and contact chairs and the community via our dedicated AE google group in case of questions or suggestions.

 ACM reproducibility badges
 Reviewers must read a paper and then thoroughly go through the Artifact Appendix to evaluate shared artifacts. They should then describe their experience at each stage (success or failure, encountered problems and how they were possibly solved, and questions or suggestions to the authors), and give a score on scale -1 .. +1:

 +1 if exceeded expectations
 0 if met expectations (or inapplicable)
 -1 if fell below expectations
 Artifacts available
 Are all artifacts related to this paper publicly available?
 Note that it is not obligatory to make artifacts publicly available!



 The author-created artifacts relevant to this paper will receive an ACM "artifact available" badge only if they have been placed on a publicly accessible archival repository such as Zenodo, FigShare, and Dryad.

 A DOI will be then assigned to their artifacts and must be provided in the Artifact Appendix!

 Notes:

 ACM does not mandate the use of above repositories. However, publisher repositories, institutional repositories, or open commercial repositories are acceptable only if they have a declared plan to enable permanent accessibility! Personal web pages, GitHub, GitLab and BitBucket are not acceptable for this purpose.
 Artifacts do not need to have been formally evaluated in order for an article to receive this badge. In addition, they need not be complete in the sense described above. They simply need to be relevant to the study and add value beyond the text in the article. Such artifacts could be something as simple as the data from which the figures are drawn, or as complex as a complete software system under study.
 The authors can provide the DOI at the very end of the AE process and use GitHub or any other convenient way to access their artifacts during AE.
 Artifacts functional
 Are all components relevant to evaluation included in the package?
 Well documented? Enough to understand, install and evaluate artifact?
 Exercisable? Includes scripts and/or software to perform appropriate experiments and generate results?
 Consistent? Artifacts are relevant to the associated paper and contribute in some inherent way to the generation of its main results?


 Note that proprietary artifacts need not be included. If they are required to exercise the package then this should be documented, along with instructions on how to obtain them. Proxies for proprietary data should be included so as to demonstrate the analysis.

 The artifacts associated with the paper will receive an "Artifacts Evaluated - Functional" badge only if they are found to be documented, consistent, complete, exercisable, and include appropriate evidence of verification and validation.

 We usually ask the authors to provide a small/sample data set to validate at least some results from the paper to make sure that their artifact is functional.

 Results reproduced
 Was it possible to validate the key results from the paper using provided artifacts?


 You should report any unexpected artifact behavior to the authors (depends on the type of artifact such as unexpected output, scalability issues, crashes, performance variation, etc).

 The artifacts associated with the paper will receive a "Results reproduced" badge only if the key results of the paper have been obtained in a subsequent study by a person or team other than the authors, using artifacts provided by the author.

 Some variation of empirical and numerical results is tolerated. In fact it is often unavoidable in computer systems research - see "how to report and compare empirical results" in the AE FAQ page, the SIGPLAN Empirical Evaluation Guidelines, and the NeurIPS reproducibility checklist.

 Since it may take weeks and even months to rerun some complex experiments such as deep learning model training, we are discussing a staged AE where we will first validate that artifacts are functional before the camera ready paper deadline, and then use a separate AE with the full validation of all experimental results with open reviewing and without strict deadlines. We successfully validated a similar approach at the MLCommons open reproducibility and optimization challenges) and there is a related initiative at the NeurIPS conference.

 Distinguished artifact award
 When arranged by the event, an artifact can receive a distinguished artifact award if it is functional, well-documented, portable, easily reproducible and reusable by the community.





 Papers that successfully go through AE will receive a set of ACM badges of approval printed on the papers themselves and available as meta information in the ACM Digital Library (it is now possible to search for papers with specific badges in ACM DL). Authors of such papers will have an option to include a two-page-max artifact appendix to their camera-ready paper. The optional artifact appendix pages will be free of charge.

ACM Reproducibility Badges
ACM Badge: Artifacts Available
Artifacts AvailableACM Badge: Artifacts Evaluated
Artifacts Evaluated — FunctionalACM Badge: Results Reproduced
Results Reproduced
Artifact Submission
An artifact submission consists of two parts:

The paper and a two-page appendix. Please prepare your appendix using the provided template. The appendix is expected to contain the following main sections:
an abstract
an itemized metainformation list
access to the artifact
system requirements and dependencies
experiment workflow
steps for evaluation
results
Note that the paper does not need to be the final version, as the main goal of this submission is to let artifact reviewers reproduce your experiments.
The artifact. Please make your artifact accessible by the reviewing committee. We do not limit the way of code delivery. However, if you would like to apply for the "Artifact Available" badge, you will need to have your artifact available at a public archival repository (for more details, see the reviewing guide).
Please submit your artifact on our submission site. When you submit, please provide details about the artifact's software and hardware requirements. This will be extremely helpful for the Artifact Evaluation Committee to find suitable reviewers.

Benefits
There are major benefits to introducing AE in our conferences.

Dissemination of Ideas: The goal of our research is to disseminate insights and encourage people to build upon that idea. Open-sourcing the artifacts and opening up the ideas to the whole community ensures that the community can work together towards solving an important problem.
Reproducibility of the Results: Artifact evaluation promotes reproducibility of experimental results and encourages code and data sharing to help the community quickly validate and compare alternative approaches.
Safeguarding the Review Process: AE incentivizes people to conduct research in an ethical manner. The recent example of misconduct in our conference reviewing process has greatly hurt the reputation of this community. Introducing AE can help to restore our integrity and commitment to reproducible and ethical research.

Frequently Asked Questions
Will my artifacts get rejected if they do not run on the first try?
AE is an iterative process between authors and reviewers. It is a positive and constructive process that makes most artifacts much stronger. The authors can revise their submission and communicate with the reviewers through the submission website.

My artifacts run on special hardware. Can I submit it?
AE supports submissions with specialized hardware and simulators. The authors provide access to their specialized hardware through the submission website. ASPLOS 2021 evaluated artifacts include FPGA prototypes, ASICs, and specialized simulators.

Some parts of my artifacts have IP restrictions. Can I submit it?
AE supports artifacts with IP restrictions. In cases where some parts of the software/hardware stack cannot be shared, we let the authors provide direct access to their platform just to our evaluators so that they can perform measurements directly on those platforms. We have several successful cases of such an approach at ASPLOS 2021. In the end, authors can still receive functional and reproduced badges, but not the available badge.