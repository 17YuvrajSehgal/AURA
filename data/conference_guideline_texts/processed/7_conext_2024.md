1. **Artifact Availability**: This factor evaluates whether the artifacts related to the paper are publicly accessible. Artifacts must be placed in a publicly accessible archival repository to receive the "Artifact Available" badge.
2. **Artifact Functionality**: This factor assesses whether all components necessary for evaluation are included, well-documented, and exercisable. Artifacts should be consistent with the paper and include scripts or software to perform experiments and generate results.
3. **Results Reproducibility**: This factor checks if the key results of the paper can be validated using the provided artifacts. It involves verifying that the results can be reproduced by an independent team using the author's artifacts.
4. **Documentation Quality**: This factor examines the clarity and comprehensiveness of the documentation provided with the artifacts. It should include installation instructions, dependencies, and a description of how to run experiments and validate results.
5. **Hardware and Software Dependencies**: This factor evaluates the specific hardware and software requirements needed to run the artifacts. It includes details about operating systems, software packages, and any special hardware needed.
6. **Experiment Workflow**: This factor assesses the description of the experimental workflow, including how experiments are implemented and executed. It should detail the steps necessary to reproduce the key results from the paper.
7. **Artifact Packaging**: This factor considers how the artifacts are packaged and made available to evaluators. Acceptable methods include online repositories, zip or tar files, Docker containers, and VM images.
8. **Public Accessibility**: This factor evaluates whether the artifacts are publicly available and if they have been archived in a repository that assigns a DOI, which is necessary for the "Artifact Available" badge.
9. **Evaluation and Expected Results**: This factor involves describing the expected results of the experiments, including any allowable variation in empirical results, to help evaluators understand what to expect when reproducing the experiments.
10. **Experiment Customization**: This optional factor considers the ability to customize and tune experiments, which can be useful for the community to explore different datasets, models, or environments.
11. **Reusability**: This factor assesses the potential for the artifacts to be reused by others, including the use of interfaces or frameworks that facilitate the adaptation and customization of experiments.
12. **Empirical Evaluation Guidelines**: This factor involves adherence to best practices for empirical evaluation, ensuring that the evidence presented in the paper is reliable and that the evaluation is thorough and well-documented.
