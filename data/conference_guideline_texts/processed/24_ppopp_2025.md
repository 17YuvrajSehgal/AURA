1. **Documentation**: The artifact must be well-documented, including a README file with a Getting Started Guide and Step-by-Step Instructions. This ensures that evaluators can understand and use the artifact effectively.
2. **Packaging**: Artifacts should be packaged in a widely available compressed archive format (e.g., ZIP, tar.gz) and should be easy to set up, ideally using a container like Docker or a virtual machine to ensure reproducibility and minimize setup overhead.
3. **Functionality**: The artifact should be functional, meaning it can be executed and performs as described in the paper. This is necessary for the 'Artifacts Evaluated — Functional' badge.
4. **Reusability**: The artifact should be reusable, allowing others to repurpose it for different settings or experiments. This is necessary for the 'Artifacts Evaluated — Reusable' badge.
5. **Results Reproduction**: The artifact should enable the reproduction of the main results of the paper, which is necessary for the 'Results Reproduced' badge.
6. **Availability**: For the 'Artifacts Available' badge, the artifact must be publicly accessible in an archival repository, ensuring long-term access and immutability.
7. **Empirical Results Variation**: The evaluation tolerates variations in empirical and numerical results, acknowledging the inherent variability in computer systems research.
8. **Anonymity and Communication**: The evaluation process is single-blind, and authors can communicate anonymously with evaluators to resolve technical issues, ensuring a fair and smooth evaluation process.
9. **Claims Support**: The artifact should clearly support the claims made in the paper, with documentation indicating which claims are supported and how.
10. **Scalability and Accuracy**: If applicable, the artifact should demonstrate scalability and maintain numerical accuracy, with scripts to report any unexpected behavior or loss in accuracy.
