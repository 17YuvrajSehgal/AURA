ACM SIGMOD 2024 Call for Artifacts
A scientific paper consists of a constellation of artifacts beyond the document itself: software, data sets, scripts, hardware, evaluation data and documentation, raw survey results, mechanized test suites, benchmarks, and so on. Often, the quality of these artifacts is as important as that of the document itself. Based on the growing success of the Availability & Reproducibility Initiative (ARI) of previous SIGMOD conferences, we will run again this year an optional artifact evaluation process. All papers presented at SIGMOD 2024 -- accepted for publication PACMMOD Vol. 1(3), Vol.1(4), Vol.2(1), and Vol.2(3) -- are encouraged to participate in the artifact evaluation process, as well as papers from the SIGMOD 2024 Industry Track.

The submission process is as follows:

Packaging Guidelines
Every case is slightly different. Sometimes the Availability & Reproducibility committee can simply rerun software (e.g., rerun some existing benchmark). At other times, obtaining raw data may require special hardware (e.g., sensors in the arctic). In the latter case, the committee will not be able to reproduce the acquisition of raw data, but then you can provide the committee with a protocol, including detailed procedures for system set-up, experiment set-up, and measurements.

Whenever raw data acquisition can be produced, the following information should be provided.

Environment
Authors should explicitly specify the OS and tools that should be installed as the environment. Such specification should include dependencies with specific hardware features (e.g., 25 GB of RAM are needed) or dependencies within the environment (e.g., the compiler that should be used must be run with a specific version of the OS).

System
System setup is one of the most challenging aspects when repeating experiments. System setup will be easier to conduct if it is automatic rather than manual. Authors should test that the system they distribute can actually be installed in a new environment. The documentation should detail every step in system setup:

How to obtain the system?
How to configure the environment if need be (e.g., environment variables, paths)?
How to compile the system? (existing compilation options should be mentioned)
How to use the system? (What are the configuration options and parameters to the system?)
How to make sure that the system is installed correctly?
The above tasks should be achieved by executing a set o scripts provided by the authors that will download needed components (systems, libraries), initialize the environment, check that software and hardware is compatible, and deploy the system.

Tools
The committee strongly suggests using one of the following tools to streamline the process of reproducibility. These tools can be used to capture the environment, the input files, the expected output files, and the required libraries in a container-like suite. This will help both the authors and the evaluators to seamlessly rerun experiments under specific environments and settings. If using all these tools proves to be difficult for a particular paper, the committee will work with the authors to find the proper solution based on the specifics of the paper and the environment needed. Below is a list of the tools recommended by the SIGMOD Reproducibility Committee.

Docker containers
ReproZip
Jupiter Notebook
GitHub repositories with clearly outlined instructions in the ReadMe file
If your artifacts require cloud deployment, we strongly suggest to create a submission using one of the open tools for reproducible science. A partial list includes:
CloudLab
Chameleon Cloud
More tools are available here: https://reproduciblescience.org/reproducibility-directory/.
Experiments
Given a system, the authors should provide the complete set of experiments to reproduce the paper's results. Typically, each experiment will consist of the following parts.

A setup phase where parameters are configured and data is loaded.
A running phase where a workload is applied and measurements are taken.
A clean-up phase where the system is prepared to avoid interference with the next round of experiments.
The authors should document (i) how to perform the setup, running and clean-up phases, and (ii) how to check that these phases complete as they should. The authors should document the expected effect of the setup phase (e.g., a cold file cache is enforced) and the different steps of the running phase, e.g., by documenting the combination of command line options used to run a given experiment script.

Experiments should be automatic, e.g., via a script that takes a range of values for each experiment parameter as arguments, rather than manual, e.g., via a script that must be edited so that a constant takes the value of a given experiment parameter.

Graphs and Plots
For each graph in the paper, the authors should describe how the graph is obtained from the experimental measurements. The submission should contain the scripts (or spreadsheets) that are used to generate the graphs. We strongly encourage authors to provide scripts for all their graphs. The authors are free to choose from their favorite plotting tool using a tool such as Gnuplot, Matlab, Matplotlib, R, or Octave.

Ideal Reproducibility Submission
At a minimum the authors should provide a complete set of scripts to install the system, produce the data, run experiments and produce the resulting graphs along with a detailed Readme file that describes the process step by step so it can be easily reproduced by a reviewer.

The ideal reproducibility submission consists of a master script that:

installs all systems needed,
generates or fetches all needed input data,
reruns all experiments and generates all results,
generates all graphs and plots, and finally,
recompiles the sources of the paper
... to produce a new PDF for the paper that contains the new graphs. It is possible!


Follow these guidelines so that an artifact associated with a paper is considered for its availability and functionality, along with the reproducibility of the paper’s key results and claims. Please see this quick guide that summarizes the key requirements and guidelines of your submission.

The artifact evaluation has two phases: a single-anonymous phase of reviewing the overall quality of the artifact and a zero-anonymous phase of reproducing the results, during which reviewers are invited to collaborate with authors. At the end of the process, for every successfully reproduced paper the reviewers (and optionally all or some of the authors) are co-authoring a reproducibility report to document the process, the core reproduced results, and any success stories, i.e., cases that during the reproducibility review the artifacts quality was improved.

All accepted SIGMOD research and industry papers are encouraged to participate in artifact evaluation.

Registration and Submission
Submitting the artifacts associated with your accepted SIGMOD paper is a two-step process.

Registration: By the artifact registration deadline, submit the abstract and PDF of your accepted SIGMOD paper, as well as topics, conflicts, and any “optional bidding instructions” for potential evaluators via the artifact submission site: https://sigmod24ari.hotcrp.com/
Submission: By the artifact submission deadline, provide a stable URL or (if that is not possible) upload an archive of your artifacts. If the URL is access-protected, provide the credentials needed to access it. Select the criteria/badges that the ARC should consider while evaluating your artifacts. You will not be able to change the URL, archive, or badge selections after the artifact submission deadline. Finally, for your artifact to be considered, check the “ready for review” box before the submission deadline.
The ARC recommends that you create a single web page at a stable URL that contains your artifact package. The ARC may contact you with questions about your artifacts as needed.

Important Dates
Artifact submission deadline: September 9, 2024
Sanity-check period: September 15 - September 25, 2024
Review Period: September 25 - November 12, 2024
Discussion period: November 12 - November 20, 2024
Final Badge Decisions: November 20, 2024
Finalize Reproducibility Reports: December 15, 2024



ACM SIGMOD 2024 Call for ARC Members
We are looking for members of the Availability and Reproducibility Committee (ARC), who will contribute to the SIGMOD 2024 Availability and Reproducibility review process by evaluating submitted artifacts. ARC membership is especially suitable for researchers early in their career, such as PhD students. Even as a first-year PhD student, you are welcome to join the ARC, provided you are working in a topic area covered by SIGMOD (broadly data management). You can be located anywhere in the world as all committee discussions will happen online.

As an ARC member, you will not only help promote the reproducibility of experimental results in systems research, but also get to familiarize yourself with research papers just accepted for publication at SIGMOD 2024 and explore their artifacts. For a given artifact, you may be asked to evaluate its public availability, functionality, and/or ability to reproduce the results from the paper. You will be able to discuss with other ARC members and interact with the authors as necessary, for instance if you are unable to get the artifact to work as expected. Finally, you will provide a review for the artifact to give constructive feedback to its authors, discuss the artifact with fellow reviewers, and help award the paper artifact evaluation badges. For all successfully reproduced artifact, you will co-author a reproducibility report with your co-reviewers and optionally the authors of the paper to document the process, the core reproduced results, and any success stories, i.e., cases that during the reproducibility review the artifacts quality was improved.

We expect that each member will evaluate 2-3 artifacts. The duration of evaluating different artifacts may vary depending on its computational cost (to be checked during the "Sanity-Check" period). ARC members are expected to allocate time to choose the artifacts they want to review, to read the chosen papers, to evaluate and review the corresponding artifacts, and to be available for online discussion until artifact notification deadline. Please ensure that you have sufficient time and availability for the ARC during the evaluation period September 10 to November 10 2024. Please also ensure you will be able to carry out the evaluation independently, without sharing artifacts or related information with others and limiting all the discussions to within the ARC.
We expect that evaluations can be done on your own computer (any moderately recent desktop or laptop computer will do). In other cases and to the extent possible, authors will arrange their artifacts so as to run in community research testbeds or will provide remote access to their systems (e.g., via SSH). Please also see this quick guide for reviewers.

How to Apply
If you are interested in taking part in the ARC, please complete this online self-nomination form.

Deadline: August 24, 2024, Anywhere on Earth

You can contact the chairs for any questions.