# AURA: Agentic Artifact Evaluation and Documentation Pipeline

## Overview

**AURA** (AI-powered Unified Research Artifact Evaluation) is a modular, conference-aware framework for automating research artifact assessment and documentation generation. It leverages a knowledge graph of conference guidelines, dynamic LLM agents, and RAG-based artifact evaluation, producing reviewer-quality reports with detailed, actionable feedback.

---

## Motivation

- **Reproducibility, transparency, and usability** of research code and data are critical to modern science.
- Conferences use diverse, nuanced criteria for artifact badges, but manual review is slow, inconsistent, and subjective.
- AURA provides an automated, data-driven, conference-aligned pipeline to evaluate, document, and grade research artifacts at scale.

---

## System Architecture

### 1. **Knowledge Graph Ingestion**
- All available conference artifact evaluation guidelines are parsed from processed markdown (`.md`) files.
- An artifact evaluation criteria table (with dimension names, keywords, weights) is also ingested.
- These are loaded into a **Neo4j knowledge graph**, with nodes and relationships:
  - `(:Conference)-[:REQUIRES_SECTION]->(:Section)`
  - `(:Conference)-[:USES_DIMENSION]->(:Dimension)`
  - `(:Dimension)-[:HAS_KEYWORD]->(:Keyword)`

### 2. **Agentic README Generation**
- For each artifact submission:
  - Conference code is selected (`ICSE 2025`, etc.).
  - The system retrieves required documentation sections and their descriptions from the knowledge graph.
  - Each section is generated by a multi-agent workflow:
    - **Author Agent:** Drafts the section using context, code, and conference guidance.
    - **Editor Agent:** Refines for clarity and completeness.
    - **Critic Agent:** Reviews each section, scoring and giving actionable feedback; low-scoring sections are revised in-loop.
  - Prompt chaining ensures context from previous sections, conference guidelines, and reviewer feedback.
- The generated README is saved for both artifact use and subsequent evaluation.

### 3. **RAG + LLM-Based Artifact Evaluation**
- For the submitted README and associated materials:
  - The system retrieves all relevant evaluation dimensions, weights, and keywords for the selected conference from Neo4j.
  - For each dimension (e.g., reproducibility, documentation, usability):
    1. **Retrieval:** Relevant passages are extracted from the README (and optionally other files) using keyword windows.
    2. **LLM Scoring:** The LLM reviews the evidence, guided by the dimension’s keywords and conference description, and produces a 0-1 score plus targeted feedback.
    3. **Weighting:** Each score is weighted by the conference’s priorities.
  - The system aggregates all dimension scores into a **final artifact grade**.

### 4. **Reviewer Report Generation**
- The pipeline produces a rich Markdown report including:
  - Final artifact score (normalized to conference scale)
  - Dimension-level score breakdown and LLM feedback
  - Full conference criteria for transparency
  - The full generated README

---

## Example Workflow

1. **Conference & Data Selection:**  
   - User selects conference code (e.g., `13_icse_2025`).
   - Artifact codebase is analyzed; structure, code, and documentation are extracted.
2. **Knowledge Graph Retrieval:**  
   - Required documentation sections, dimension weights, and keywords are loaded from Neo4j.
3. **Agentic README Generation:**  
   - Each required section is generated, reviewed, and iteratively improved by the LLM agents using prompt chaining.
4. **RAG-based Evaluation:**  
   - For each dimension, relevant README evidence is retrieved, and the LLM rates coverage and provides feedback.
   - Weighted scores are aggregated.
5. **Report Output:**  
   - Final score, full feedback, and the README are saved in a Markdown (or HTML/PDF) report for both authors and reviewers.

---

## Extensibility

- **New conferences**: Add new guideline `.md` and criteria CSV; pipeline ingests automatically.
- **Custom agents**: Add new reviewer/editor/critic roles as nodes in the workflow.
- **Full artifact evaluation**: Extend to support additional file types or execute code/tests.
- **Web API/UI**: Wrap as a service for artifact badge reviews, journal workflows, or reproducibility audits.

---

## Example Output Report

See `reviewer_report.md` for a sample evaluation including:

- Final artifact grade (e.g., `0.89/1.0`)
- Table of dimension-wise scores and LLM-generated feedback
- Sections/keywords most responsible for any point deductions
- The full, LLM-generated README

---

## Citation

If you use or extend AURA, please cite:

